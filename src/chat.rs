use std::io;

use async_openai::error::OpenAIError;
use lazy_static::lazy_static;
use dotenv_codegen::dotenv;
use async_openai::Client;
use thiserror::Error;
use async_openai::types::{
  ChatCompletionRequestMessage, ChatCompletionRequestSystemMessage, ChatCompletionRequestSystemMessageArgs, ChatCompletionRequestUserMessage, ChatCompletionRequestUserMessageArgs, CreateChatCompletionRequestArgs
};

use crate::config;

lazy_static! {
  static ref MAX_LENGTH: u8 = dotenv!("MAX_LENGTH").parse::<u8>().unwrap();
  static ref TIMEOUT: u64 = dotenv!("TIMEOUT").parse::<u64>().unwrap();
  static ref LANGUAGE: String = dotenv!("LANGUAGE").to_string();
  static ref MODEL: String = dotenv!("MODEL").to_string();
}

#[derive(Error, Debug)]
pub enum ChatError {
  #[error("Failed to build HTTP client")]
  HttpClientBuildError,
  #[error("HTTP error: {0}")]
  HttpRequestError(#[from] reqwest::Error),
  #[error("IO error: {0}")]
  IOError(#[from] io::Error),
  #[error("Failed to parse JSON: {0}")]
  JsonParseError(#[from] serde_json::Error),
  #[error("Anyhow error: {0}")]
  Anyhow(#[from] anyhow::Error),
  #[error("OpenAI error: {0}")]
  OpenAIError(String),
  #[error("Failed to parse response: {1} ({0})")]
  ParseError(serde_json::Error, String),
  #[error("OpenAI error: {0}")]
  OpenAI(#[from] OpenAIError)
}

fn history_prompt(git_history: String, no_commits: u8) -> Result<ChatCompletionRequestUserMessage, OpenAIError> {
  let payload = format!(
    "
    The following is a summary of prior interactions generated by an AI model
    This historical context is provided to inform the AI's response to the current query.
    The history is generated using map & reduce from the last {no_commits} commits.
    History: {git_history}
  "
  )
  .split_whitespace()
  .collect::<Vec<&str>>()
  .join(" ");

  ChatCompletionRequestUserMessageArgs::default().content(payload).build()
}

fn system_prompt(language: String, max_length_of_commit: usize) -> Result<ChatCompletionRequestSystemMessage, OpenAIError> {
  let payload = format!(
    "
    Create a concise git commit message in present tense for the provided code diff.
    Follow these guidelines:
    * Language: {language}.
    * Maximum Length: {max_length_of_commit} characters.
    * Clearly detail what changes were made and why.
    * Exclude irrelevant and unnecessary details, such as translations.
    Your entire response will be passed directly into git commit:
  "
  )
  .split_whitespace()
  .collect::<Vec<&str>>()
  .join(" ");

  // TODO: Check out the options
  ChatCompletionRequestSystemMessageArgs::default().content(payload).build()
}

fn user_prompt(diff: String) -> Result<ChatCompletionRequestUserMessage, OpenAIError> {
  let payload = format!(
    "
    Here is the latest code diff for analysis:
    ======= BEGIN DIFF =======
    {diff}
    ====== END DIFF ======
    Please generate a commit message based on this diff.
  "
  )
  .split_whitespace()
  .collect::<Vec<&str>>()
  .join(" ");

  ChatCompletionRequestUserMessageArgs::default().content(payload).build()
}

fn history() -> Option<(String, u8)> {
  None
}

pub async fn generate_commit(diff: String) -> Result<String, ChatError> {
  log::info!("Generating commit message using config: {:?}", config::APP);

  let max_length_of_commit = config::APP.max_length;
  let max_tokens = config::APP.max_diff_tokens;
  let language = config::APP.language.clone();
  let model = config::APP.model.clone();
  let timeout = config::APP.duration();

  let mut messages: Vec<ChatCompletionRequestMessage> =
    vec![system_prompt(language, max_length_of_commit)?.into(), user_prompt(diff)?.into()];

  if let Some((git_history, no_commits)) = history() {
    messages.insert(1, history_prompt(git_history, no_commits)?.into());
  }

  log::info!("Sending request to OpenAI API: {:?}", messages);

  log::info!("Using backoff timeout of {:?}", timeout);
  let backoff = backoff::ExponentialBackoffBuilder::new().with_max_elapsed_time(Some(timeout)).build();
  let client = Client::new().with_backoff(backoff);

  log::info!("Creating chat completion request");
  let request = CreateChatCompletionRequestArgs::default()
    .max_tokens(max_tokens as u16)
    .messages(messages)
    .model(model)
    .n(1)
    .build()?;

  log::info!("Sending request to OpenAI API");
  client
    .chat()
    .create(request)
    .await?
    .choices
    .first()
    .and_then(|choice| choice.message.content.clone())
    .ok_or_else(|| ChatError::OpenAIError("Failed to get response from OpenAI".to_string()))
}
