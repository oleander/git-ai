use std::io;

use anyhow::Context;
// use serde_json::{from_str, json, Value};
// use serde::{Deserialize, Serialize};
use lazy_static::lazy_static;
use dotenv_codegen::dotenv;
use thiserror::Error;
// use reqwest::Client;
use async_openai::{
  config::AzureConfig, error::OpenAIError, types::{
    ChatCompletionRequestMessage, ChatCompletionRequestSystemMessage, ChatCompletionRequestSystemMessageArgs, ChatCompletionRequestUserMessage, ChatCompletionRequestUserMessageArgs, CreateChatCompletionRequestArgs, CreateEditRequestArgs, CreateEmbeddingRequestArgs
  }, Client
};

use crate::config;

// const API_URL: &str = "https://api.openai.com/v1/chat/completions";

lazy_static! {
  static ref MAX_LENGTH: u8 = dotenv!("MAX_LENGTH").parse::<u8>().unwrap();
  static ref TIMEOUT: u64 = dotenv!("TIMEOUT").parse::<u64>().unwrap();
  static ref LANGUAGE: String = dotenv!("LANGUAGE").to_string();
  static ref MODEL: String = dotenv!("MODEL").to_string();
}

#[derive(Error, Debug)]
pub enum ChatError {
  #[error("Failed to build HTTP client")]
  HttpClientBuildError,
  #[error("HTTP error: {0}")]
  HttpRequestError(#[from] reqwest::Error),
  #[error("IO error: {0}")]
  IOError(#[from] io::Error),
  #[error("Failed to parse JSON: {0}")]
  JsonParseError(#[from] serde_json::Error),
  // #[error("Failed to extract message from response body")]
  // ResponseExtractionError,
  #[error("Anyhow error: {0}")]
  Anyhow(#[from] anyhow::Error),
  #[error("OpenAI error: {0}")]
  OpenAIError(String),
  #[error("Failed to parse response: {1} ({0})")]
  ParseError(serde_json::Error, String),
  #[error("OpenAI error: {0}")]
  OpenAI(#[from] OpenAIError)
}

// fn payload(diff: String) -> Value {
//   let model = config::APP.model.clone();

//   json!({
//     "model": model,
//     "messages": vec![
//       json!({
//         "role": "system",
//         "content": prompt()
//       }),
//       json!({
//         "role": "user",
//         "content": diff
//       })
//     ]
//   })
// }

fn history_prompt(git_history: String, no_commits: u8) -> Result<ChatCompletionRequestUserMessage, OpenAIError> {
  let payload = format!(
    "
    The following is a summary of prior interactions generated by an AI model
    This historical context is provided to inform the AI's response to the current query.
    The history is generated using map & reduce from the last {no_commits} commits.
    History: {git_history}
  "
  )
  .split_whitespace()
  .collect::<Vec<&str>>()
  .join(" ");

  ChatCompletionRequestUserMessageArgs::default().content(payload).build()
}

// let lang = config::APP.language.clone();
// let length = config::APP.max_length;
fn system_prompt(language: String, max_length_of_commit: usize) -> Result<ChatCompletionRequestSystemMessage, OpenAIError> {
  let payload = format!(
    "
    Create a concise git commit message in present tense for the provided code diff.
    Follow these guidelines:
    * Language: {language}.
    * Maximum Length: {max_length_of_commit} characters.
    * Clearly detail what changes were made and why.
    * Exclude irrelevant and unnecessary details, such as translations.
    Your entire response will be passed directly into git commit:
  "
  )
  .split_whitespace()
  .collect::<Vec<&str>>()
  .join(" ");

  // TODO: Check out the options
  ChatCompletionRequestSystemMessageArgs::default().content(payload).build()
}

fn user_prompt(diff: String) -> Result<ChatCompletionRequestUserMessage, OpenAIError> {
  let payload = format!(
    "
    Here is the latest code diff for analysis:
    ======= BEGIN DIFF =======
    {diff}
    ====== END DIFF ======
    Please generate a commit message based on this diff.
  "
  )
  .split_whitespace()
  .collect::<Vec<&str>>()
  .join(" ");

  ChatCompletionRequestUserMessageArgs::default().content(payload).build()
}

// mod response {
//   use super::*;

//   #[derive(Debug, Serialize, Deserialize)]
//   pub struct Success {
//     system_fingerprint: String,
//     pub choices:        Vec<Choice>,
//     object:             String,
//     model:              String,
//     id:                 String,
//     usage:              Usage
//   }

//   #[derive(Debug, Serialize, Deserialize)]
//   pub struct Error {
//     pub error:   String,
//     code:        usize,
//     pub message: String
//   }

//   #[derive(Debug, Serialize, Deserialize)]
//   pub struct Usage {
//     completion_tokens: usize,
//     prompt_tokens:     usize,
//     total_tokens:      usize
//   }

//   #[derive(Debug, Serialize, Deserialize)]
//   pub struct Choice {
//     finish_reason: String,
//     index:         usize,
//     pub message:   Message
//   }

//   #[derive(Debug, Serialize, Deserialize)]
//   pub struct Message {
//     pub content: String,
//     role:        String
//   }
// }

// #[derive(Debug, Serialize, Deserialize)]
// #[serde(untagged)]
// pub enum Response {
//   Success(response::Success),
//   Error(response::Error)
// }

fn history() -> Option<(String, u8)> {
  None
}

async fn response(diff: String) -> Result<String, ChatError> {
  let api_key = config::APP
    .openai_api_key
    .clone()
    .context("Failed to get OpenAI API key, please run `git-ai config set openapi-api-key <api-key>`")?;
  let language = config::APP.language.clone();
  let timeout = config::APP.duration();
  let model = config::APP.model.clone();
  let max_tokens = config::APP.max_diff_tokens;
  let max_length_of_commit = config::APP.max_length;

  let mut messages: Vec<ChatCompletionRequestMessage> =
    vec![system_prompt(language, max_length_of_commit)?.into(), user_prompt(diff)?.into()];

  if let Some((git_history, no_commits)) = history() {
    messages.insert(1, history_prompt(git_history, no_commits)?.into());
  }

  let backoff = backoff::ExponentialBackoffBuilder::new().with_max_elapsed_time(Some(timeout)).build();

  let client = Client::new().with_backoff(backoff);

  let request = CreateChatCompletionRequestArgs::default()
    .max_tokens(max_tokens as u16)
    .messages(messages)
    .model(model)
    .n(1)
    .build()?;

  client
    .chat()
    .create(request)
    .await?
    .choices
    .first()
    .and_then(|choice| choice.message.content.clone())
    .ok_or_else(|| ChatError::OpenAIError("Failed to get response from OpenAI".to_string()))
}

// pub async fn generate_commit(diff: String) -> Result<String, ChatError> {

// }
